{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2fed0c33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fed0c33",
        "outputId": "d8f79ed2-fe92-424c-b681-9564b45d585f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Streaming Language Modeling Data Pipeline with Hugging Face Datasets\n",
            "--------------------------------------------------------------------\n",
            "Goal:\n",
            "    Demonstrate how to build a *true streaming* LM pipeline that:\n",
            "    - Processes data without loading the entire dataset into RAM.\n",
            "    - Tokenizes on the fly.\n",
            "    - Concatenates text and chunks into fixed-length blocks for LM training.\n",
            "    - Produces batches ready for training in PyTorch.\n",
            "\n",
            "Key Teaching Points:\n",
            "    1. Streaming allows us to work with web-scale corpora.\n",
            "    2. We still can do grouping/chunking in a rolling fashion.\n",
            "    3. This approach mimics real-world pipelines for large-scale LM training.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Streaming Language Modeling Data Pipeline with Hugging Face Datasets\n",
        "--------------------------------------------------------------------\n",
        "Goal:\n",
        "    Demonstrate how to build a *true streaming* LM pipeline that:\n",
        "    - Processes data without loading the entire dataset into RAM.\n",
        "    - Tokenizes on the fly.\n",
        "    - Concatenates text and chunks into fixed-length blocks for LM training.\n",
        "    - Produces batches ready for training in PyTorch.\n",
        "\n",
        "Key Teaching Points:\n",
        "    1. Streaming allows us to work with web-scale corpora.\n",
        "    2. We still can do grouping/chunking in a rolling fashion.\n",
        "    3. This approach mimics real-world pipelines for large-scale LM training.\n",
        "\"\"\"\n",
        "print(__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0ae182f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ae182f4",
        "outputId": "3cf95ce1-7441-4b03-d735-a5ffbe5031ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "190f9fe4",
      "metadata": {
        "id": "190f9fe4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "04603b35",
      "metadata": {
        "id": "04603b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
         
        },
        "outputId": "74eba5e9-0e4b-4651-cb41-bb9f31e52a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading roneneldan/TinyStories in streaming mode...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acd540f33c8e4e29bdbf219e0e1216c6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. Load the dataset in STREAMING mode\n",
        "# ============================================================\n",
        "# TinyStories is a larger synthetic dataset, making streaming essential.\n",
        "# We still use streaming=True to avoid RAM overload.\n",
        "dataset_name = \"roneneldan/TinyStories\"\n",
        "print(f\"Loading {dataset_name} in streaming mode...\")\n",
        "\n",
        "stream_dataset = load_dataset(\n",
        "    dataset_name,\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "36818657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "41f1c9c1ac344acfa465404540f60575",
            "34eccbcdf98b405bb226560bd5db0d53",
            "3637576eb60f4747858f5641e382cc08",
            "e99886503a604ac28895b47f01edf3bc",
            "426f31eadfb84578abb6ca4e29c3b49e",
            "6336f55a713f4087b083e2addf7b8f24",
            "5238e59a06a6475ab932ddc4fbf7e194",
            "bfdbc563e0634ce1893305606db7bee1",
            "ad35236946764e14b3944cdcde3518fc",
            "9932cb594c3c4dba86e43e734f5c9b7f",
            "7496ef1af2d24ffaa88557ebe5a90382",
            "90ce661f6dac4cfcbb27ee796bddeb31",
            "7424c7a2dac145b4889641cf92632b4a",
            "bc4e5d2ecb3542fd8ae00c8a929254b8",
            "71e81a57d77345a78d4662a0e0498deb",
            "0c76b96b0b444005b155f033e18ce545",
            "31e9abbecba74c4bafcabe59b8961751",
            "4fa3709b5c9c48629f1ff0f1b9590931",
            "b4d51c6ca22149408a983aa62acf5449",
            "969063cf2d3a4042836f25b37b8f48df",
            "b1ba37e3fcb547e99285b919f594bf11",
            "48aedcc913a94f8186e49cc1af52b33b",
            "26bfba2d1d2d44f28e4b0cc9c9295e30",
            "3719773ef07d476babde01dedb88e2c5",
            "8ed881970c0745968bf6f744e24681c2",
            "0f45e4599c184a69acb7cf9c5bb7bfcb",
            "b842caf5b24140f59e8717a924e8c5be",
            "5bc156edbe234749ab7283c3333c51d7",
            "ee63c73ad748469c9eea82981a130fcf",
            "0d3ecd627d5a4f4d87a0149374e2af65",
            "a653b230e2b0412b897ad5b62c96191a",
            "ed019c3e52fc48519bf0d8581b6ba470",
            "3c59a441619a407e83c9bbe37e35f78e"
          ]
        },
        "id": "36818657",
        "outputId": "53decd98-2943-4f20-cfe9-cac647836f45"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41f1c9c1ac344acfa465404540f60575"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90ce661f6dac4cfcbb27ee796bddeb31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26bfba2d1d2d44f28e4b0cc9c9295e30"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. Initialize the tokenizer\n",
        "# ============================================================\n",
        "# Pythia is a modern model suite often used for research.\n",
        "# We load its specific tokenizer here.\n",
        "model_checkpoint = \"EleutherAI/pythia-70m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Ensure pad token is defined (Pythia/GPT-NeoX usually uses eos_token as pad)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "09ee19e3",
      "metadata": {
        "id": "09ee19e3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. Tokenization step\n",
        "# ============================================================\n",
        "# We do NOT pad/truncate here â€” we want raw token sequences.\n",
        "# This keeps flexibility to later concatenate across documents.\n",
        "def tokenize_function(examples):\n",
        "    # TinyStories also uses a \"text\" column, so this remains compatible\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized_stream = stream_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d4779cf2",
      "metadata": {
        "id": "d4779cf2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. Rolling buffer for grouping into fixed-length blocks\n",
        "# ============================================================\n",
        "# Because streaming datasets are iterators, we can't look ahead arbitrarily.\n",
        "# We'll keep a buffer that stores leftover tokens from the previous batch,\n",
        "# so we can concatenate and chunk consistently.\n",
        "block_size = 256\n",
        "\n",
        "def group_texts_streaming(dataset_iter, block_size):\n",
        "    buffer = []\n",
        "    for example in dataset_iter:\n",
        "        buffer.extend(example[\"input_ids\"])\n",
        "        while len(buffer) >= block_size:\n",
        "            chunk = buffer[:block_size]\n",
        "            buffer = buffer[block_size:]\n",
        "            yield {\n",
        "                \"input_ids\": chunk,\n",
        "                # Create a simple attention mask (1s for real tokens)\n",
        "                \"attention_mask\": [1] * block_size\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "51d766de",
      "metadata": {
        "id": "51d766de"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. Wrap generator in an IterableDataset\n",
        "# ============================================================\n",
        "class StreamingLMIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_iterable_dataset, block_size):\n",
        "        self.dataset = hf_iterable_dataset\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return group_texts_streaming(self.dataset, self.block_size)\n",
        "\n",
        "grouped_iterable_dataset = StreamingLMIterableDataset(tokenized_stream, block_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "60b9f8fe",
      "metadata": {
        "id": "60b9f8fe"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. Collate function for batches\n",
        "# ============================================================\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in batch], dtype=torch.long)\n",
        "\n",
        "    # For Causal LM (like Pythia/GPT), labels are usually the input_ids\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": input_ids.clone()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3990a633",
      "metadata": {
        "id": "3990a633"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. DataLoader for streaming data\n",
        "# ============================================================\n",
        "train_loader = DataLoader(grouped_iterable_dataset, batch_size=8, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b43f40b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b43f40b7",
        "outputId": "ef25de4f-26aa-4a83-8b2a-7bc809241841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stream ready. Model: EleutherAI/pythia-70m | Dataset: roneneldan/TinyStories\n",
            "Sample streaming batches:\n",
            "Batch 0 -> input_ids shape: torch.Size([8, 256])\n",
            "Batch 1 -> input_ids shape: torch.Size([8, 256])\n",
            "Batch 2 -> input_ids shape: torch.Size([8, 256])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 8. Iterate over a few batches\n",
        "# ============================================================\n",
        "print(f\"\\nStream ready. Model: {model_checkpoint} | Dataset: {dataset_name}\")\n",
        "print(\"Sample streaming batches:\")\n",
        "\n",
        "for i, batch in enumerate(train_loader):\n",
        "    print(f\"Batch {i} -> input_ids shape: {batch['input_ids'].shape}\")\n",
        "    # Verify the shapes match our new block_size of 256\n",
        "    if i == 2:\n",
        "        break"
      ]
    }
  ],
"metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
